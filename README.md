# Adversarial Framing Robustness Benchmark (AFRB)

The Adversarial Framing Robustness Benchmark (AFRB) is a novel evaluation framework designed to  assess AI systems' resilience to adversarial framing and manipulation through roleplay scenarios. AFRB contains two complementary components: (1) a framing susceptibility assessment that tests AI vulnerability to varied phrasings of identical questions, and (2) a roleplay component featuring escalating ethical dilemmas in multi-turn decision trees. By unifying these approaches, AFRB provides a comprehensive measurement of how language models maintain ethical consistency, boundary enforcement, and decision stability when exposed to subtle manipulation techniques. This benchmark addresses a critical gap in AI safety evaluation by focusing on vulnerability to context manipulation, which is a major concern in high-stakes real-world applications where subtle framing changes could influence models to produce harmful or misleading outputs.
